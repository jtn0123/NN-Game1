# ğŸ§  Neural Network Game AI - Atari Breakout

A complete, educational implementation of a Deep Q-Learning (DQN) agent that learns to play Atari Breakout **in real-time** with a **live neural network visualizer**.

![Project Architecture](docs/architecture.png)

---

## ğŸ“‹ Table of Contents

1. [Project Overview](#-project-overview)
2. [Architecture Deep Dive](#-architecture-deep-dive)
3. [Installation](#-installation)
4. [Quick Start](#-quick-start)
5. [How It Works](#-how-it-works)
6. [Configuration Guide](#-configuration-guide)
7. [Extending to Other Games](#-extending-to-other-games)
8. [Troubleshooting](#-troubleshooting)

---

## ğŸ¯ Project Overview

### What This Project Does

This project demonstrates **reinforcement learning** by training a neural network to play Breakout:

1. **The Game** (`src/game/`) - A complete Atari Breakout implementation
2. **The AI Brain** (`src/ai/`) - Deep Q-Network (DQN) that learns to play
3. **The Visualizer** (`src/visualizer/`) - Real-time neural network activity visualization

### Key Features

- âœ… **Live Training Visualization** - Watch neurons fire as the AI learns
- âœ… **Real-time Gameplay** - See the AI play the game live
- âœ… **Training Metrics Dashboard** - Loss curves, rewards, epsilon decay
- âœ… **Modular Architecture** - Easy to swap games or AI algorithms
- âœ… **Configurable Hyperparameters** - Tune learning rate, network size, etc.
- âœ… **Checkpoint System** - Save/load trained models

---

## ğŸ—ï¸ Architecture Deep Dive

### Project Structure

```
NN-Game1/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ config.py                    # All hyperparameters & settings
â”œâ”€â”€ main.py                      # Entry point - run this!
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ game/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ breakout.py          # Game logic (state, physics, scoring)
â”‚   â”‚   â””â”€â”€ renderer.py          # Pygame rendering
â”‚   â”‚
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ network.py           # Neural network architecture (PyTorch)
â”‚   â”‚   â”œâ”€â”€ agent.py             # DQN agent (action selection, learning)
â”‚   â”‚   â”œâ”€â”€ replay_buffer.py     # Experience replay memory
â”‚   â”‚   â””â”€â”€ trainer.py           # Training loop orchestration
â”‚   â”‚
â”‚   â””â”€â”€ visualizer/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ nn_visualizer.py     # Neural network visualization
â”‚       â””â”€â”€ dashboard.py         # Training metrics display
â”‚
â”œâ”€â”€ models/                      # Saved model checkpoints
â”‚   â””â”€â”€ .gitkeep
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_game.py
    â”œâ”€â”€ test_agent.py
    â””â”€â”€ test_network.py
```

### Component Interaction Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              MAIN TRAINING LOOP                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                           â”‚                           â”‚
        â–¼                           â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               â”‚          â”‚               â”‚          â”‚               â”‚
â”‚   BREAKOUT    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   DQN AGENT   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  VISUALIZER   â”‚
â”‚     GAME      â”‚          â”‚               â”‚          â”‚               â”‚
â”‚               â”‚          â”‚               â”‚          â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                           â”‚                           â”‚
        â”‚                           â”‚                           â”‚
        â–¼                           â–¼                           â–¼
   Game State               Neural Network              Live Display
   - Ball position          - Input Layer (84)          - Network graph
   - Paddle position        - Hidden (256, 128)         - Activations
   - Brick states           - Output (3 actions)        - Metrics
   - Score/Lives            - Q-values                  - Game view
```

---

## ğŸ”§ Installation

### Prerequisites

- Python 3.9+ (tested with 3.11)
- pip package manager

### Setup Steps

```bash
# 1. Clone or navigate to the project
cd /Users/justin/Documents/Github/NN-Game1

# 2. Create a virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Verify installation
python -c "import torch; import pygame; print('Ready!')"
```

---

## ğŸš€ Quick Start

### Watch AI Learn from Scratch

```bash
python main.py --mode train --visualize
```

### Load Pre-trained Model

```bash
python main.py --mode play --model models/breakout_best.pth
```

### Train Without Visualization (Faster)

```bash
python main.py --mode train --headless
```

---

## ğŸ“š How It Works

### 1. The Game: Breakout (`src/game/breakout.py`)

The game maintains a **state** that the AI reads:

```python
# State representation (what the AI "sees")
state = {
    'ball_x': float,        # Ball X position (normalized 0-1)
    'ball_y': float,        # Ball Y position (normalized 0-1)
    'ball_dx': float,       # Ball X velocity (normalized)
    'ball_dy': float,       # Ball Y velocity (normalized)
    'paddle_x': float,      # Paddle X position (normalized 0-1)
    'bricks': [0,1,1,...],  # Binary array: 0=broken, 1=exists
}
```

**Actions the AI can take:**
- `0` = Move paddle LEFT
- `1` = Stay (no movement)
- `2` = Move paddle RIGHT

### 2. The AI Brain: Deep Q-Network (`src/ai/network.py`)

#### What is Q-Learning?

The AI learns a **Q-function** that estimates "how good" each action is:

```
Q(state, action) = Expected future reward if I take this action
```

#### Network Architecture

```
INPUT LAYER                 HIDDEN LAYERS                 OUTPUT LAYER
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ State       â”‚            â”‚  256 neurons â”‚               â”‚ Q(s, LEFT)  â”‚
â”‚ (84 values) â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  ReLU        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Q(s, STAY)  â”‚
â”‚             â”‚            â”‚  128 neurons â”‚               â”‚ Q(s, RIGHT) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  ReLU        â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Input (84 values):**
- Ball position (2)
- Ball velocity (2)
- Paddle position (1)
- Brick states (80 bricks = 80 values, or however many bricks you have)

**Output (3 values):**
- Q-value for LEFT, STAY, RIGHT

The AI picks the action with the **highest Q-value**.

### 3. Learning: Experience Replay & Target Network

#### Why Experience Replay?

Instead of learning from just the last experience, we store experiences in a **replay buffer** and sample random batches. This provides:
- âœ… Better sample efficiency
- âœ… Breaks correlation between consecutive samples
- âœ… More stable learning

```python
# Experience tuple
experience = (state, action, reward, next_state, done)

# Training step
batch = replay_buffer.sample(batch_size=64)
loss = compute_td_loss(batch)
optimizer.step()
```

#### Target Network (Stability)

We maintain TWO copies of the network:
1. **Policy Network** - Updated every step
2. **Target Network** - Updated periodically (every 1000 steps)

This prevents the "moving target" problem where Q-value estimates chase themselves.

### 4. Exploration vs Exploitation (Îµ-greedy)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Epsilon (Îµ) starts at 1.0 (100% random exploration)   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚
â”‚  Epsilon decays to 0.01 (mostly exploitation)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

if random() < epsilon:
    action = random_action()      # EXPLORE: try random things
else:
    action = best_q_value_action  # EXPLOIT: use learned policy
```

### 5. The Visualizer (`src/visualizer/nn_visualizer.py`)

The visualizer shows:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   INPUT       â”‚    â”‚   HIDDEN      â”‚    â”‚   OUTPUT      â”‚       â”‚
â”‚  â”‚   LAYER       â”‚    â”‚   LAYER       â”‚    â”‚   LAYER       â”‚       â”‚
â”‚  â”‚               â”‚    â”‚               â”‚    â”‚               â”‚       â”‚
â”‚  â”‚   â—‹ â—‹ â—‹ â—‹    â”‚â”€â”€â”€â–ºâ”‚   â—‰ â—‹ â—‰ â—‹    â”‚â”€â”€â”€â–ºâ”‚   â—‰ LEFT     â”‚       â”‚
â”‚  â”‚   â—‹ â—‹ â—‹ â—‹    â”‚    â”‚   â—‹ â—‰ â—‹ â—‰    â”‚    â”‚   â—‹ STAY     â”‚       â”‚
â”‚  â”‚   â—‹ â—‹ â—‹ â—‹    â”‚    â”‚   â—‰ â—‹ â—‰ â—‹    â”‚    â”‚   â—‹ RIGHT    â”‚       â”‚
â”‚  â”‚   ...        â”‚    â”‚   ...        â”‚    â”‚               â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                      â”‚
â”‚  â—‰ = Active (high activation)    â—‹ = Inactive (low activation)    â”‚
â”‚  Line thickness = connection weight strength                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Configuration Guide

All hyperparameters are in `config.py`:

```python
# Learning Parameters
LEARNING_RATE = 0.0001        # Higher = faster but unstable
GAMMA = 0.99                   # Discount factor (0.95-0.99)
BATCH_SIZE = 64               # Samples per training step

# Exploration
EPSILON_START = 1.0           # Initial exploration rate
EPSILON_END = 0.01            # Minimum exploration rate
EPSILON_DECAY = 0.995         # Decay rate per episode

# Network Architecture
HIDDEN_LAYERS = [256, 128]    # Neurons per hidden layer

# Training
TARGET_UPDATE = 1000          # Steps between target network updates
MEMORY_SIZE = 100000          # Replay buffer capacity
SAVE_EVERY = 100              # Episodes between checkpoints
```

### Tuning Tips

| Symptom | Try This |
|---------|----------|
| AI doesn't learn | Increase `LEARNING_RATE`, check reward function |
| Learning is unstable | Decrease `LEARNING_RATE`, increase `BATCH_SIZE` |
| AI gets stuck in local minimum | Increase `EPSILON_DECAY`, increase exploration |
| Training too slow | Decrease `HIDDEN_LAYERS` size, use GPU |

---

## ğŸ® Extending to Other Games

This architecture is designed to be **game-agnostic**. To add a new game:

### 1. Create a New Game Class

```python
# src/game/your_game.py
from src.game.base_game import BaseGame

class YourGame(BaseGame):
    def __init__(self):
        self.state_size = ...    # How many input values
        self.action_size = ...   # How many possible actions
    
    def reset(self) -> np.ndarray:
        """Reset game and return initial state"""
        pass
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool]:
        """Execute action, return (next_state, reward, done)"""
        pass
    
    def render(self, screen):
        """Draw game to pygame screen"""
        pass
```

### 2. Register in Config

```python
# config.py
GAME = "your_game"  # Changed from "breakout"
```

### 3. That's It!

The agent, visualizer, and training loop work with any game that follows the `BaseGame` interface.

---

## ğŸ› Troubleshooting

### Common Issues

**"CUDA out of memory"**
```bash
# Force CPU training
python main.py --device cpu
```

**"Pygame window not responding"**
- Make sure you're not running in a headless environment
- Try: `export SDL_VIDEODRIVER=x11`

**"Training seems stuck"**
- Check if epsilon is decaying (should decrease over time)
- Verify rewards are non-zero (add print statements)
- Try different random seeds: `python main.py --seed 42`

**"AI only moves one direction"**
- Reward function may be imbalanced
- Try longer training (at least 1000 episodes)
- Check state normalization

---

## ğŸ“Š Understanding the Metrics

During training, you'll see:

```
Episode: 100 | Score: 4 | Avg: 2.5 | Îµ: 0.45 | Loss: 0.023
        â”‚       â”‚        â”‚         â”‚         â”‚
        â”‚       â”‚        â”‚         â”‚         â””â”€â”€ TD Error (should decrease)
        â”‚       â”‚        â”‚         â””â”€â”€ Exploration rate (should decrease)
        â”‚       â”‚        â””â”€â”€ Running average score (should increase)
        â”‚       â””â”€â”€ Score this episode
        â””â”€â”€ Episode number
```

### What Good Training Looks Like

1. **Episodes 1-100:** Random movement, low scores, high exploration
2. **Episodes 100-500:** AI starts tracking ball, scores improve
3. **Episodes 500-1000:** Consistent improvement, exploration drops
4. **Episodes 1000+:** Mastery, high scores, minimal exploration

---

## ğŸ¤ Contributing

This is a learning project! Feel free to:
- Add new games
- Improve visualizations
- Try different RL algorithms (A3C, PPO, etc.)
- Optimize performance

---

## ğŸ“œ License

MIT License - Use this for learning and teaching!

---

## ğŸ™ Acknowledgments

- DeepMind's DQN paper (Mnih et al., 2015)
- OpenAI Gym for inspiration
- Pygame community

---

**Happy Learning! ğŸš€**

